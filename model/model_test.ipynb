{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction\n",
    "\n",
    "### 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_WIDTH    = 50\n",
    "SCREEN_HEIGHT   = 20\n",
    "FONT_SIZE       = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Size of convolution layer\n",
    "CLASStorch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input: (N, C_in, H_in, W_in)\n",
    "\n",
    "output: (N, C_out, H_out, W_out)\n",
    "\n",
    "$$H_{out} = \\frac{H_{in} + 2 \\times padding[0] - dilation[0] \\times (kernel\\_size[0] -1) -1}{stride[0]}+1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, out_channels=6, kernel_size=1, stride=1, padding=0, vocab_size=5000, linear_size=5000, normalization=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=out_channels, \n",
    "            kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # self.fc1 = nn.Linear(3000, vocab_size) \n",
    "        self.fc1 = nn.Linear(linear_size, vocab_size) \n",
    "\n",
    "        # self.cnn = nn.Sequential(\n",
    "        #     nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"original shape:\", x.shape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        # print(\"after C shape:\", x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(\"after view:\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "render image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hanya\\\\OneDrive\\\\Informatik\\\\LMU\\\\Informatik-2023SS\\\\CS\\\\CV\\\\group_project\\\\cvdl_ss23\\\\model'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert input string to img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "# SCREEN_WIDTH    = 100\n",
    "# SCREEN_HEIGHT   = 20\n",
    "# FONT_SIZE       = 10 \n",
    "\n",
    "SCREEN_WIDTH    = 130\n",
    "SCREEN_HEIGHT   = 25\n",
    "FONT_SIZE       = 15 \n",
    "# 4680\n",
    "\n",
    "pygame.init()\n",
    "font_noto_sans_regular = pygame.font.Font(\"../converttext/noto-sans.regular.ttf\", FONT_SIZE)\n",
    "\n",
    "\n",
    "def to_image(text:str, font, id:int=None, noise=False):\n",
    "  # pygame.init()\n",
    "  # screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "  screen = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "  screen.fill((255, 255, 255))\n",
    "  # draw image\n",
    "  img = font.render(str(text), True, (0, 0, 0))\n",
    "  screen.blit(img, (2, 0))\n",
    "  for event in pygame.event.get():\n",
    "    if event.type == pygame.QUIT:\n",
    "      run = False\n",
    "  # pygame.display.flip() \n",
    "  # Save the screen as an image when the program finishes\n",
    "  if noise == False:\n",
    "    filename = f\"./temp_image/word_{str(id)}_{str(text)}_notoSans.png\"\n",
    "  else:\n",
    "    filename = f\"./temp_image/word_{str(id)}_{str(text)}_notoSans_noised.png\"\n",
    "  pygame.image.save(screen, filename)\n",
    "  # print(\"Screen saved as \", filename)\n",
    "  # pygame.quit()\n",
    "  return filename\n",
    "\n",
    "image_path = to_image(text=\"1nd1st1nguishÎ±ble\", font=font_noto_sans_regular, id=5, noise=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer (discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "# paths = [\"../dataset/ted_dev_en-de.raw.en.txt\"]\n",
    "# tokenizer.train(files=paths, vocab_size=52_000, min_frequency=1, special_tokens=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.token_to_id(\"so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dict:\n",
    "\n",
    "word -> id\n",
    "\n",
    "id -> word\n",
    "\n",
    "word -> count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# id_to_word_list = []                    # list: id   -> word\n",
    "# word_num_dict   = defaultdict(int)      # dict: word -> num(word)\n",
    "# word_to_id_dict = defaultdict(int)      # dict: word -> id\n",
    "# id_to_word_dict = defaultdict(str)      # dict: id   -> word\n",
    "# with open(\"../dataset/ted_dev_en-de.raw.en.txt\", 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         if line != '\\n':\n",
    "#             words = line.strip()\n",
    "#             tokens = word_tokenize(words)\n",
    "#             for token in tokens:\n",
    "#                 if token.isalpha():\n",
    "#                     if token not in id_to_word_list:\n",
    "#                         id_to_word_list.append(token)\n",
    "#                     word_num_dict[token] +=1\n",
    "\n",
    "# for id, word in enumerate(id_to_word_list):\n",
    "#     word_to_id_dict[word] = id\n",
    "#     id_to_word_dict[id]   = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"length of word_num_dict:\", len(word_num_dict.keys()))\n",
    "# print(\"length of id_to_word_dict:\", len(id_to_word_dict.keys()))\n",
    "# print(\"length of word_id_dict:\", len(word_to_id_dict.keys()))\n",
    "# print(f\"index: [5], word in id_to_word_dict: [{id_to_word_dict[5]}], id in word_id_dict: [{word_to_id_dict[id_to_word_dict[5]]}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dicts\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('word_num_dict.json', 'w') as fp:\n",
    "#     json.dump(word_num_dict, fp)\n",
    "# with open('word_to_id_dict.json', 'w') as fp:\n",
    "#     json.dump(word_to_id_dict, fp)\n",
    "# with open('id_to_word_dict.json', 'w') as fp:\n",
    "#     json.dump(id_to_word_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get dataset and its lenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test load: length of word_num_dict: 4572\n",
      "test load: length of id_to_word_dict: 4572\n",
      "test load: length of word_id_dict: 4572\n",
      "note index needs to use str(index)\n",
      "test load: index: [5], word in id_to_word_dict: [defines], id in word_id_dict: [5]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "with open('word_num_dict.json', 'r') as fp:\n",
    "    word_num_dict_test = json.load(fp)\n",
    "with open('word_to_id_dict.json', 'r') as fp:\n",
    "    word_to_id_dict_test = json.load(fp)\n",
    "with open('id_to_word_dict.json', 'r') as fp:\n",
    "    id_to_word_dict_test = json.load(fp)\n",
    "    \n",
    "print(\"test load: length of word_num_dict:\", len(word_num_dict_test.keys()))\n",
    "print(\"test load: length of word_to_id_dict_test:\", len(word_to_id_dict_test.keys()))\n",
    "print(\"test load: length of id_to_word_dict_test:\", len(id_to_word_dict_test.keys()))\n",
    "print('note index needs to use str(index)')\n",
    "print(f\"test load: index: [5], word in id_to_word_dict: [{id_to_word_dict_test[str(5)]}], id in word_id_dict: [{word_to_id_dict_test[id_to_word_dict_test['5']]}]\")\n",
    "\n",
    "VOCAB_SIZE = len(word_num_dict_test.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get max length of imput words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "things\n",
      "defines\n",
      "stewardship\n",
      "preservation\n",
      "unfortunately\n",
      "counterintuitive\n",
      "indistinguishable\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# lst = list(word_num_dict_test.keys())\n",
    "# m = 0\n",
    "# for word in lst:\n",
    "#     if len(word) > m:\n",
    "#         m = len(word)\n",
    "#         print(word)\n",
    "# print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize cuda and model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(out_channels=16, kernel_size=3, stride=1, padding=1, vocab_size=VOCAB_SIZE, normalization=False, linear_size=12480).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from torchvision import transforms\n",
    "\n",
    "# transform_norm = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # image_path = \"../converttext/imgdataset/5AMPLETyponoised.jpg\"\n",
    "# img = Image.open(image_path).convert('L')\n",
    "# dog = transform_norm(img)\n",
    "# # dog = normalize(dog)\n",
    "# dog = torch.unsqueeze(dog, 0)\n",
    "# output = model(dog.to(device))\n",
    "# print(output)\n",
    "# pred = torch.nn.Softmax(output)\n",
    "# print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset {word:str, image:path, label:int}\n",
    "\n",
    "see: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "class WordImageIDDataset(Dataset):\n",
    "    def __init__(self, word_to_id_list, font, noise=False):\n",
    "        self.word_to_id_list = word_to_id_list\n",
    "        self.font = font\n",
    "        self.noise = noise\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word_to_id_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ''' index is not token ID '''\n",
    "        output_word = self.word_to_id_list[index][0]\n",
    "        image_path = to_image(\n",
    "            text=output_word, \n",
    "            font=self.font,\n",
    "            id=index, \n",
    "            noise=self.noise)\n",
    "        \n",
    "        # for path in image_paths:\n",
    "        output_img = Image.open(image_path).convert('L')\n",
    "        output_img = transform_norm(output_img)\n",
    "        id = self.word_to_id_list[index][1]\n",
    "        # output_id_onehot = torch.zeros(1, VOCAB_SIZE)\n",
    "        # output_id_onehot[0][id] = 1\n",
    "        output_id_onehot = torch.zeros(VOCAB_SIZE)\n",
    "        output_id_onehot[id] = 1\n",
    "                \n",
    "        output = {'word'    : output_word,\n",
    "                  'image'   : output_img,\n",
    "                  'id'      : output_id_onehot}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(list(word_to_id_dict_test.items()))\n",
    "# print(list(word_to_id_dict_test.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try pipeline without dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "\n",
    "# transform_norm = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# for word, id in word_to_id_dict_test.items():\n",
    "    \n",
    "#     # id to target\n",
    "#     target = torch.zeros(1, VOCAB_SIZE)\n",
    "#     target[0][id] = 1\n",
    "    \n",
    "#     # word to image\n",
    "#     image_path = to_image(word, font=font_noto_sans_regular, id=id, noise=False)\n",
    "#     img = Image.open(image_path).convert('L')\n",
    "#     img = transform_norm(img)\n",
    "#     img = torch.unsqueeze(img, 0)\n",
    "#     output = model(img.to(device))\n",
    "#     # print(output)\n",
    "#     softmax = torch.nn.Softmax()\n",
    "#     pred = softmax(output)\n",
    "#     print(pred) \n",
    "#     loss = criterion(pred, target.to(device))\n",
    "#     print(\"Loss:\", loss.item())\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the model\n",
    "try pipline with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "8.226514782938924\n",
      "epoch: 1\n",
      "7.60472496572908\n",
      "epoch: 2\n",
      "5.519001911570142\n",
      "epoch: 3\n",
      "1.9904476673036189\n",
      "epoch: 4\n",
      "0.7794529113110963\n",
      "epoch: 5\n",
      "0.3541651229289445\n",
      "epoch: 6\n",
      "0.18261963604473985\n",
      "epoch: 7\n",
      "0.10275357683886077\n",
      "epoch: 8\n",
      "0.06643135929838397\n",
      "epoch: 9\n",
      "0.04610278003383428\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "dataset = WordImageIDDataset(word_to_id_list = list(word_to_id_dict_test.items()),\n",
    "                            font=font_noto_sans_regular,\n",
    "                            noise=False)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    epoch_loss = []\n",
    "    for data in dataloader:\n",
    "        words    = data['word']\n",
    "        imgs     = data['image']\n",
    "        ids      = data['id']\n",
    "        optimizer.zero_grad()\n",
    "        # print(word)\n",
    "        # print(imgs.shape)\n",
    "        # print(id)\n",
    "        # print(ids.shape)\n",
    "        # read image    \n",
    "        outputs = model(imgs.to(device))\n",
    "        # print(output)\n",
    "        softmax = torch.nn.Softmax()\n",
    "        # outputs = softmax(outputs)\n",
    "        # print(pred) \n",
    "        # print(preds.shape)\n",
    "        # print(ids.shape)\n",
    "        \n",
    "        loss = criterion(outputs, ids.to(device))\n",
    "        # print(\"Loss:\", loss.item())\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # torch.cuda.empty_cache()\n",
    "    print(sum(epoch_loss)/len(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./CNN_130_250_15_16_3_1_1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test with just one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original word:\tdefinitely\n",
      "prediction:\t definitely\n"
     ]
    }
   ],
   "source": [
    "word = \"definitely\"\n",
    "true_id = word_to_id_dict_test[word]\n",
    "\n",
    "word_image_path = f\"./temp_image/word_4571_definitely_notoSans.png\"\n",
    "\n",
    "# img_path = to_image(word, font=font_noto_sans_regular)\n",
    "img = Image.open(word_image_path).convert('L')\n",
    "img = transform_norm(img)           # 16, 25, 130. 16: num of feature maps\n",
    "img = torch.unsqueeze(img, 0)       # 1, 16, 25, 160. 1: batch, 16: num feature maps\n",
    "# print(img.shape)\n",
    "\n",
    "output = model(img.to(device))\n",
    "# print(output)\n",
    "pred = torch.argmax(output)\n",
    "# print(pred)\n",
    "print(f\"original word:\\t{word}\")\n",
    "print(\"prediction:\\t\", id_to_word_dict_test[str(pred.item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original word:\tdefinitely\n",
      "noised word:\tdef1n1te1y\n",
      "prediction:\t deficiencies\n"
     ]
    }
   ],
   "source": [
    "word = \"definitely\"\n",
    "true_id = word_to_id_dict_test[word]\n",
    "word_noise = \"def1n1te1y\"\n",
    "\n",
    "print(f\"original word:\\t{word}\")\n",
    "print(f\"noised word:\\t{word_noise}\")\n",
    "\n",
    "img_path = to_image(word_noise, font=font_noto_sans_regular)\n",
    "img = Image.open(img_path).convert('L')\n",
    "img = transform_norm(img)           # 16, 25, 130. 16: num of feature maps\n",
    "img = torch.unsqueeze(img, 0)       # 1, 16, 25, 160. 1: batch, 16: num feature maps\n",
    "# print(img.shape)\n",
    "\n",
    "output = model(img.to(device))\n",
    "# print(output)\n",
    "pred = torch.argmax(output)\n",
    "# print(pred)\n",
    "\n",
    "print(\"prediction:\\t\", id_to_word_dict_test[str(pred.item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original word:\tdefinitely\n",
      "noised word:\tdef1nitely\n",
      "prediction:\t described\n"
     ]
    }
   ],
   "source": [
    "word = \"definitely\"\n",
    "true_id = word_to_id_dict_test[word]\n",
    "word_noise = \"def1nitely\"\n",
    "\n",
    "print(f\"original word:\\t{word}\")\n",
    "print(f\"noised word:\\t{word_noise}\")\n",
    "\n",
    "img_path = to_image(word_noise, font=font_noto_sans_regular)\n",
    "img = Image.open(img_path).convert('L')\n",
    "img = transform_norm(img)           # 16, 25, 130. 16: num of feature maps\n",
    "img = torch.unsqueeze(img, 0)       # 1, 16, 25, 160. 1: batch, 16: num feature maps\n",
    "# print(img.shape)\n",
    "\n",
    "output = model(img.to(device))\n",
    "# print(output)\n",
    "pred = torch.argmax(output)\n",
    "# print(pred)\n",
    "\n",
    "print(\"prediction:\\t\", id_to_word_dict_test[str(pred.item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original word:\tdefinitely\n",
      "noised word:\tdefinite1y\n",
      "prediction:\t definitely\n"
     ]
    }
   ],
   "source": [
    "word = \"definitely\"\n",
    "true_id = word_to_id_dict_test[word]\n",
    "word_noise = \"definite1y\"\n",
    "\n",
    "print(f\"original word:\\t{word}\")\n",
    "print(f\"noised word:\\t{word_noise}\")\n",
    "img_path = to_image(word_noise, font=font_noto_sans_regular)\n",
    "img = Image.open(img_path).convert('L')\n",
    "img = transform_norm(img)           # 16, 25, 130. 16: num of feature maps\n",
    "img = torch.unsqueeze(img, 0)       # 1, 16, 25, 160. 1: batch, 16: num feature maps\n",
    "# print(img.shape)\n",
    "\n",
    "output = model(img.to(device))\n",
    "# print(output)\n",
    "pred = torch.argmax(output)\n",
    "# print(pred)\n",
    "\n",
    "print(\"prediction:\\t\", id_to_word_dict_test[str(pred.item())])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvss23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
