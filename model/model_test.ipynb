{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape of image:\n",
    "\n",
    "```\n",
    "SCREEN_WIDTH = 450\n",
    "SCREEN_HEIGHT = 200\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_WIDTH = 450\n",
    "SCREEN_HEIGHT = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASStorch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input: (N, C_in, H_in, W_in)\n",
    "\n",
    "output: (N, C_out, H_out, W_out)\n",
    "\n",
    "$$H_{out} = \\frac{H_{in} + 2 \\times padding[0] - dilation[0] \\times (kernel\\_size[0] -1) -1}{stride[0]}+1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\University\\Code_LMU\\GAN-BERT-Classifier\\py39env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, out_channels=6, kernel_size=1, stride=1, padding=0, vocab_size=5000, normalization=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=out_channels, \n",
    "            kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(135000, vocab_size) \n",
    "\n",
    "        # self.cnn = nn.Sequential(\n",
    "        #     nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"original shape:\", x.shape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        # print(\"after C shape:\", x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(\"after view:\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "render image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hanya\\\\OneDrive\\\\Informatik\\\\LMU\\\\Informatik-2023SS\\\\CS\\\\CV\\\\group_project\\\\cvdl_ss23\\\\model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "\n",
    "SCREEN_WIDTH = 450\n",
    "SCREEN_HEIGHT = 200\n",
    "pygame.init()\n",
    "font_noto_sans_regular = pygame.font.Font(\"../converttext/noto-sans.regular.ttf\", 70)\n",
    "\n",
    "\n",
    "def to_image(text:str, font, id:int=None, noise=False):\n",
    "  # pygame.init()\n",
    "  # screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "  screen = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "  screen.fill((255, 255, 255))\n",
    "  # draw image\n",
    "  img = font.render(str(text), True, (0, 0, 0))\n",
    "  screen.blit(img, (30, 60))\n",
    "  for event in pygame.event.get():\n",
    "    if event.type == pygame.QUIT:\n",
    "      run = False\n",
    "  # pygame.display.flip() \n",
    "  # Save the screen as an image when the program finishes\n",
    "  if noise == False:\n",
    "    filename = f\"./temp_image/word_{str(id)}_{str(text)}_notoSans.jpg\"\n",
    "  else:\n",
    "    filename = f\"./temp_image/word_{str(id)}_{str(text)}_notoSans_noised.jpg\"\n",
    "  pygame.image.save(screen, filename)\n",
    "  # print(\"Screen saved as \", filename)\n",
    "  # pygame.quit()\n",
    "  return filename\n",
    "\n",
    "image_path = to_image(text=\"difficult\", font=font_noto_sans_regular, id=5, noise=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer (discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "# paths = [\"../dataset/ted_dev_en-de.raw.en.txt\"]\n",
    "# tokenizer.train(files=paths, vocab_size=52_000, min_frequency=1, special_tokens=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.token_to_id(\"so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dict:\n",
    "\n",
    "word -> id\n",
    "\n",
    "id -> word\n",
    "\n",
    "word -> count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# id_to_word_list = []                    # list: id   -> word\n",
    "# word_num_dict   = defaultdict(int)      # dict: word -> num(word)\n",
    "# word_to_id_dict = defaultdict(int)      # dict: word -> id\n",
    "# id_to_word_dict = defaultdict(str)      # dict: id   -> word\n",
    "# with open(\"../dataset/ted_dev_en-de.raw.en.txt\", 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         if line != '\\n':\n",
    "#             words = line.strip()\n",
    "#             tokens = word_tokenize(words)\n",
    "#             for token in tokens:\n",
    "#                 if token.isalpha():\n",
    "#                     if token not in id_to_word_list:\n",
    "#                         id_to_word_list.append(token)\n",
    "#                     word_num_dict[token] +=1\n",
    "\n",
    "# for id, word in enumerate(id_to_word_list):\n",
    "#     word_to_id_dict[word] = id\n",
    "#     id_to_word_dict[id]   = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"length of word_num_dict:\", len(word_num_dict.keys()))\n",
    "# print(\"length of id_to_word_dict:\", len(id_to_word_dict.keys()))\n",
    "# print(\"length of word_id_dict:\", len(word_to_id_dict.keys()))\n",
    "# print(f\"index: [5], word in id_to_word_dict: [{id_to_word_dict[5]}], id in word_id_dict: [{word_to_id_dict[id_to_word_dict[5]]}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dicts\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('word_num_dict.json', 'w') as fp:\n",
    "#     json.dump(word_num_dict, fp)\n",
    "# with open('word_to_id_dict.json', 'w') as fp:\n",
    "#     json.dump(word_to_id_dict, fp)\n",
    "# with open('id_to_word_dict.json', 'w') as fp:\n",
    "#     json.dump(id_to_word_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test load: length of word_num_dict: 4572\n",
      "test load: length of id_to_word_dict: 4572\n",
      "test load: length of word_id_dict: 4572\n",
      "note index needs to use str(index)\n",
      "test load: index: [5], word in id_to_word_dict: [defines], id in word_id_dict: [5]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "with open('word_num_dict.json', 'r') as fp:\n",
    "    word_num_dict_test = json.load(fp)\n",
    "with open('word_to_id_dict.json', 'r') as fp:\n",
    "    word_to_id_dict_test = json.load(fp)\n",
    "with open('id_to_word_dict.json', 'r') as fp:\n",
    "    id_to_word_dict_test = json.load(fp)\n",
    "    \n",
    "print(\"test load: length of word_num_dict:\", len(word_num_dict_test.keys()))\n",
    "print(\"test load: length of id_to_word_dict:\", len(word_to_id_dict_test.keys()))\n",
    "print(\"test load: length of word_id_dict:\", len(word_to_id_dict_test.keys()))\n",
    "print('note index needs to use str(index)')\n",
    "print(f\"test load: index: [5], word in id_to_word_dict: [{id_to_word_dict_test[str(5)]}], id in word_id_dict: [{word_to_id_dict_test[id_to_word_dict_test['5']]}]\")\n",
    "\n",
    "VOCAB_SIZE = len(word_num_dict_test.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(out_channels=6, kernel_size=1, stride=1, padding=0, vocab_size=VOCAB_SIZE, normalization=False).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from torchvision import transforms\n",
    "\n",
    "# transform_norm = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # image_path = \"../converttext/imgdataset/5AMPLETyponoised.jpg\"\n",
    "# img = Image.open(image_path).convert('L')\n",
    "# dog = transform_norm(img)\n",
    "# # dog = normalize(dog)\n",
    "# dog = torch.unsqueeze(dog, 0)\n",
    "# output = model(dog.to(device))\n",
    "# print(output)\n",
    "# pred = torch.nn.Softmax(output)\n",
    "# print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset {word:str, image:path, label:int}\n",
    "\n",
    "see: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "class WordImageIDDataset(Dataset):\n",
    "    def __init__(self, word_to_id_list, font, noise=False):\n",
    "        self.word_to_id_list = word_to_id_list\n",
    "        self.font = font\n",
    "        self.noise = noise\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word_to_id_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ''' index is not token ID '''\n",
    "        output_word = self.word_to_id_list[index][0]\n",
    "        image_path = to_image(\n",
    "            text=output_word, \n",
    "            font=self.font,\n",
    "            id=index, \n",
    "            noise=self.noise)\n",
    "        \n",
    "        # for path in image_paths:\n",
    "        output_img = Image.open(image_path).convert('L')\n",
    "        output_img = transform_norm(output_img)\n",
    "        id = self.word_to_id_list[index][1]\n",
    "        # output_id_onehot = torch.zeros(1, VOCAB_SIZE)\n",
    "        # output_id_onehot[0][id] = 1\n",
    "        output_id_onehot = torch.zeros(VOCAB_SIZE)\n",
    "        output_id_onehot[id] = 1\n",
    "                \n",
    "        output = {'word'    : output_word,\n",
    "                  'image'   : output_img,\n",
    "                  'id'      : output_id_onehot}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(list(word_to_id_dict_test.items()))\n",
    "# print(list(word_to_id_dict_test.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try pipeline without dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "\n",
    "# transform_norm = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# for word, id in word_to_id_dict_test.items():\n",
    "    \n",
    "#     # id to target\n",
    "#     target = torch.zeros(1, VOCAB_SIZE)\n",
    "#     target[0][id] = 1\n",
    "    \n",
    "#     # word to image\n",
    "#     image_path = to_image(word, font=font_noto_sans_regular, id=id, noise=False)\n",
    "#     img = Image.open(image_path).convert('L')\n",
    "#     img = transform_norm(img)\n",
    "#     img = torch.unsqueeze(img, 0)\n",
    "#     output = model(img.to(device))\n",
    "#     # print(output)\n",
    "#     softmax = torch.nn.Softmax()\n",
    "#     pred = softmax(output)\n",
    "#     print(pred) \n",
    "#     loss = criterion(pred, target.to(device))\n",
    "#     print(\"Loss:\", loss.item())\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try pipline with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 200, 450])\n",
      "torch.Size([1, 4572])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanya\\AppData\\Local\\Temp\\ipykernel_109204\\2651273405.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds = softmax(outputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4572])\n",
      "torch.Size([1, 4572])\n",
      "Loss: 8.427752494812012\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.30 GiB (GPU 0; 6.00 GiB total capacity; 4.60 GiB already allocated; 0 bytes free; 4.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \n\u001b[0;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 30\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     32\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32md:\\Code\\University\\Code_LMU\\GAN-BERT-Classifier\\py39env\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\Code\\University\\Code_LMU\\GAN-BERT-Classifier\\py39env\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32md:\\Code\\University\\Code_LMU\\GAN-BERT-Classifier\\py39env\\lib\\site-packages\\torch\\optim\\adam.py:218\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    212\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[0;32m    213\u001b[0m     torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m,), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mp\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mfused\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    215\u001b[0m     \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.\u001b[39m)\n\u001b[0;32m    216\u001b[0m )\n\u001b[0;32m    217\u001b[0m \u001b[39m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mexp_avg\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros_like(p, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format)\n\u001b[0;32m    219\u001b[0m \u001b[39m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m    220\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mexp_avg_sq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(p, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.30 GiB (GPU 0; 6.00 GiB total capacity; 4.60 GiB already allocated; 0 bytes free; 4.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "dataset = WordImageIDDataset(word_to_id_list = list(word_to_id_dict_test.items()),\n",
    "                            font=font_noto_sans_regular,\n",
    "                            noise=False)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for data in dataloader:\n",
    "    words    = data['word']\n",
    "    imgs     = data['image']\n",
    "    ids      = data['id']\n",
    "    # print(word)\n",
    "    print(imgs.shape)\n",
    "    # print(id)\n",
    "    print(ids.shape)\n",
    "    # read image    \n",
    "    outputs = model(imgs.to(device))\n",
    "    # print(output)\n",
    "    softmax = torch.nn.Softmax()\n",
    "    preds = softmax(outputs)\n",
    "    # print(pred) \n",
    "    print(preds.shape)\n",
    "    print(ids.shape)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(preds, ids.to(device))\n",
    "    print(\"Loss:\", loss.item())\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39env",
   "language": "python",
   "name": "py39env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
