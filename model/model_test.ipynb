{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape of image:\n",
    "\n",
    "```\n",
    "SCREEN_WIDTH = 450\n",
    "SCREEN_HEIGHT = 200\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_WIDTH = 450\n",
    "SCREEN_HEIGHT = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASStorch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input: (N, C_in, H_in, W_in)\n",
    "\n",
    "output: (N, C_out, H_out, W_out)\n",
    "\n",
    "$$H_{out} = \\frac{H_{in} + 2 \\times padding[0] - dilation[0] \\times (kernel\\_size[0] -1) -1}{stride[0]}+1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\University\\Code_LMU\\GAN-BERT-Classifier\\py39env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, out_channels=6, kernel_size=1, stride=1, padding=0, vocab_size=5000, normalization=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=out_channels, \n",
    "            kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(135000, vocab_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"original shape:\", x.shape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        print(\"after C shape:\", x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(\"after view:\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "render image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screen saved as  ./temp_image/d1fferenÅ“_notoSans.jpg\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "pygame.init()\n",
    "SCREEN_WIDTH = 450\n",
    "SCREEN_HEIGHT = 200\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "font_noto_sans_regular = pygame.font.Font(\"../converttext/noto-sans.regular.ttf\", 70)\n",
    "\n",
    "# pygame.quit()\n",
    "def to_image(input, font, noise=False):\n",
    "  screen.fill((255, 255, 255))\n",
    "  # draw image\n",
    "  img = font.render(input, True, (0, 0, 0))\n",
    "  screen.blit(img, (30, 60))\n",
    "  for event in pygame.event.get():\n",
    "    if event.type == pygame.QUIT:\n",
    "      run = False\n",
    "  pygame.display.flip() \n",
    "  # Save the screen as an image when the program finishes\n",
    "  if noise == False:\n",
    "    filename = \"./temp_image/\" + input + \"_notoSans.jpg\"\n",
    "  else:\n",
    "    filename = \"./temp_image/\" + input + \"_noto_noised.jpg\"\n",
    "  pygame.image.save(screen, filename)\n",
    "  print(\"Screen saved as \", filename)\n",
    "  return filename\n",
    "\n",
    "image_path = to_image(input, font=font_noto_sans_regular, noise=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "# paths = [\"../dataset/ted_dev_en-de.raw.en.txt\"]\n",
    "# tokenizer.train(files=paths, vocab_size=52_000, min_frequency=1, special_tokens=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.token_to_id(\"so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dict:\n",
    "\n",
    "word -> id\n",
    "\n",
    "id -> word\n",
    "\n",
    "word -> count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# id_to_word_list = []                    # list: id   -> word\n",
    "# word_num_dict   = defaultdict(int)      # dict: word -> num(word)\n",
    "# word_to_id_dict = defaultdict(int)      # dict: word -> id\n",
    "# id_to_word_dict = defaultdict(str)      # dict: id   -> word\n",
    "# with open(\"../dataset/ted_dev_en-de.raw.en.txt\", 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         if line != '\\n':\n",
    "#             words = line.strip()\n",
    "#             tokens = word_tokenize(words)\n",
    "#             for token in tokens:\n",
    "#                 if token.isalpha():\n",
    "#                     if token not in id_to_word_list:\n",
    "#                         id_to_word_list.append(token)\n",
    "#                     word_num_dict[token] +=1\n",
    "\n",
    "# for id, word in enumerate(id_to_word_list):\n",
    "#     word_to_id_dict[word] = id\n",
    "#     id_to_word_dict[id]   = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"length of word_num_dict:\", len(word_num_dict.keys()))\n",
    "# print(\"length of id_to_word_dict:\", len(id_to_word_dict.keys()))\n",
    "# print(\"length of word_id_dict:\", len(word_to_id_dict.keys()))\n",
    "# print(f\"index: [5], word in id_to_word_dict: [{id_to_word_dict[5]}], id in word_id_dict: [{word_to_id_dict[id_to_word_dict[5]]}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dicts\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('word_num_dict.json', 'w') as fp:\n",
    "#     json.dump(word_num_dict, fp)\n",
    "# with open('word_to_id_dict.json', 'w') as fp:\n",
    "#     json.dump(word_to_id_dict, fp)\n",
    "# with open('id_to_word_dict.json', 'w') as fp:\n",
    "#     json.dump(id_to_word_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test load: length of word_num_dict: 4572\n",
      "test load: length of id_to_word_dict: 4572\n",
      "test load: length of word_id_dict: 4572\n",
      "note index needs to use str(index)\n",
      "test load: index: [5], word in id_to_word_dict: [defines], id in word_id_dict: [5]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "with open('word_num_dict.json', 'r') as fp:\n",
    "    word_num_dict_test = json.load(fp)\n",
    "with open('word_to_id_dict.json', 'r') as fp:\n",
    "    word_to_id_dict_test = json.load(fp)\n",
    "with open('id_to_word_dict.json', 'r') as fp:\n",
    "    id_to_word_dict_test = json.load(fp)\n",
    "    \n",
    "print(\"test load: length of word_num_dict:\", len(word_num_dict_test.keys()))\n",
    "print(\"test load: length of id_to_word_dict:\", len(word_to_id_dict_test.keys()))\n",
    "print(\"test load: length of word_id_dict:\", len(word_to_id_dict_test.keys()))\n",
    "print('note index needs to use str(index)')\n",
    "print(f\"test load: index: [5], word in id_to_word_dict: [{id_to_word_dict_test[str(5)]}], id in word_id_dict: [{word_to_id_dict_test[id_to_word_dict_test['5']]}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(out_channels=6, kernel_size=1, stride=1, padding=0, vocab_size=VOCAB_SIZE, normalization=False).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape: torch.Size([1, 3, 200, 450])\n",
      "after C shape: torch.Size([1, 6, 100, 225])\n",
      "after view: torch.Size([1, 135000])\n",
      "tensor([[-0.1080, -0.1653, -0.4257,  ...,  0.3773, -0.0302, -0.2515]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# image_path = \"../converttext/imgdataset/5AMPLETyponoised.jpg\"\n",
    "img = Image.open(image_path)\n",
    "dog = transform_norm(img)\n",
    "# dog = normalize(dog)\n",
    "dog = torch.unsqueeze(dog, 0)\n",
    "output = model(dog.to(device))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 4572"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
